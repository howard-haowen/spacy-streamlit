import pandas as pd
import re
import requests 
import spacy
from spacy_streamlit import visualize_ner, visualize_tokens
#from spacy.language import Language
from spacy.tokens import Doc
import spacy_ke
import streamlit as st

# Global variables
DEFAULT_TEXT = """VivÃ­ asÃ­, solo, nadie con quien poder hablar verdaderamente, hasta cuando hace seis aÃ±os tuve una averÃ­a en el desierto de Sahara. Algo se habÃ­a estropeado en el motor. Como no llevaba conmigo ni mecÃ¡nico ni pasajero alguno, me dispuse a realizar, yo solo, una reparaciÃ³n difÃ­cil. Era para mÃ­ una cuestiÃ³n de vida o muerte, pues apenas tenÃ­a agua de beber para ocho dÃ­as.

La primera noche me dormÃ­ sobre la arena, a unas mil millas de distancia del lugar habitado mÃ¡s prÃ³ximo. Estaba mÃ¡s aislado que un nÃ¡ufrago en una balsa en medio del ocÃ©ano. ImagÃ­nense, pues, mi sorpresa cuando al amanecer me despertÃ³ una extraÃ±a vocecita que decÃ­a:
â€” Â¡Por favor... pÃ­ntame un cordero!
â€”Â¿Eh?
â€”Â¡PÃ­ntame un cordero!

EL PRINCIPITO
"""
DESCRIPTION = "AIæ¨¡å‹è¼”åŠ©èªè¨€å­¸ç¿’ï¼šè¥¿èª"
TOK_SEP = " | "
MODEL_NAME = "es_core_news_sm"
API_LOOKUP = {}
MAX_SYM_NUM = 5
es_vocab_df = pd.read_csv("./es_vocab.csv",encoding="utf-8")

# External API caller
def free_dict_caller(word):
    req = requests.get(f"https://api.dictionaryapi.dev/api/v2/entries/en/{word}")
    try:
        result = req.json()[0]
        if word not in API_LOOKUP:
            API_LOOKUP[word] = result
    except:
        pass
 
def show_definitions(word, pos):
    if pos == 'ADV':
        pos = 'adverb'
    if word not in API_LOOKUP:
        free_dict_caller(word)
    
    result = API_LOOKUP.get(word)
    if result:
        meanings = result.get('meanings')
        if meanings:
            definitions = []
            for meaning in meanings:
                if meaning['partOfSpeech'] == pos.lower():
                    definitions = meaning.get('definitions')

            if len(definitions) > 3:
              definitions = definitions[:3]
            
            for definition in definitions:
              df = definition.get("definition")
              st.markdown(f" - {df}")
              st.markdown("---")  
                  
    else:
        st.info("Found no matching result on Free Dictionary!")

def get_synonyms(word, pos):
    if word not in API_LOOKUP:
        free_dict_caller(word)
    
    result = API_LOOKUP.get(word)
    if result:
        meanings = result.get('meanings')
        if meanings:
            synonyms = []
            for meaning in meanings:
                if meaning['partOfSpeech'] == pos.lower():
                    synonyms = meaning.get('synonyms')
            return synonyms
        
# Utility functions
def create_eng_df(tokens):
    seen_texts = []
    filtered_tokens = []
    for tok in tokens:
        if tok.lemma_ not in seen_texts:
            filtered_tokens.append(tok)

    df = pd.DataFrame(
      {
          "å–®è©": [tok.text.lower() for tok in filtered_tokens],
          "è©é¡": [tok.pos_ for tok in filtered_tokens],
          "åŸå½¢": [tok.lemma_ for tok in filtered_tokens],
      }
    )
    st.dataframe(df)
    csv = df.to_csv().encode('utf-8')
    st.download_button(
      label="ä¸‹è¼‰è¡¨æ ¼",
      data=csv,
      file_name='span_forms.csv',
      )

def filter_tokens(doc):
    clean_tokens = [tok for tok in doc if tok.pos_ not in ["PUNCT", "SYM"]]
    clean_tokens = [tok for tok in clean_tokens if not tok.like_email]
    clean_tokens = [tok for tok in clean_tokens if not tok.like_url]
    clean_tokens = [tok for tok in clean_tokens if not tok.like_num]
    clean_tokens = [tok for tok in clean_tokens if not tok.is_punct]
    clean_tokens = [tok for tok in clean_tokens if not tok.is_space]
    return clean_tokens

def create_kw_section(doc):
    st.markdown("## é—œéµè©åˆ†æ") 
    kw_num = st.slider("è«‹é¸æ“‡é—œéµè©æ•¸é‡", 1, 10, 3)
    kws2scores = {keyword: score for keyword, score in doc._.extract_keywords(n=kw_num)}
    kws2scores = sorted(kws2scores.items(), key=lambda x: x[1], reverse=True)
    count = 1
    for keyword, score in kws2scores: 
        rounded_score = round(score, 3)
        st.write(f"{count} >>> {keyword} ({rounded_score})")
        count += 1 

# Page setting
st.set_page_config(
    page_icon="ğŸ¤ ",
    layout="wide",
    initial_sidebar_state="auto",
)
st.markdown(f"# {DESCRIPTION}") 

# Load the language model
nlp = spacy.load(MODEL_NAME)

# Add pipelines to spaCy
nlp.add_pipe("yake") # keyword extraction
# nlp.add_pipe("merge_entities") # Merge entity spans to tokens

# Page starts from here
st.markdown("## å¾…åˆ†ææ–‡æœ¬")     
st.info("è«‹åœ¨ä¸‹é¢çš„æ–‡å­—æ¡†è¼¸å…¥æ–‡æœ¬ä¸¦æŒ‰ä¸‹Ctrl + Enterä»¥æ›´æ–°åˆ†æçµæœ")
text = st.text_area("",  DEFAULT_TEXT, height=200)
doc = nlp(text)
st.markdown("---")

st.info("è«‹å‹¾é¸ä»¥ä¸‹è‡³å°‘ä¸€é …åŠŸèƒ½")
keywords_extraction = st.checkbox("é—œéµè©åˆ†æ", False)
# analyzed_text = st.checkbox("å¢å¼·æ–‡æœ¬", True)
defs_examples = st.checkbox("å–®è©è§£æ", True)
morphology = st.checkbox("è©å½¢è®ŠåŒ–", False)
ner_viz = st.checkbox("å‘½åå¯¦é«”", True)
tok_table = st.checkbox("æ–·è©ç‰¹å¾µ", False)

if keywords_extraction:
    create_kw_section(doc)

# if analyzed_text:
#     st.markdown("## åˆ†æå¾Œæ–‡æœ¬")     
#     for idx, sent in enumerate(doc.sents):
#         enriched_sentence = []
#         for tok in sent:
#             if tok.pos_ != "VERB":
#                 enriched_sentence.append(tok.text)
#             else:
#                 synonyms = get_synonyms(tok.text, tok.pos_)
#                 if synonyms:
#                     if len(synonyms) > MAX_SYM_NUM:
#                         synonyms = synonyms[:MAX_SYM_NUM]
#                     added_verbs = " | ".join(synonyms)
#                     enriched_tok = f"{tok.text} (cf. {added_verbs})"
#                     enriched_sentence.append(enriched_tok)  
#                 else:
#                     enriched_sentence.append(tok.text)

#         display_text = " ".join(enriched_sentence)
#         st.write(f"{idx+1} >>> {display_text}")     

if defs_examples:
    st.markdown("## å–®è©è§£é‡‹")
    clean_tokens = filter_tokens(doc)
    ez_token = []
    words = []
    for token in clean_tokens:
        if len(es_vocab_df.index[es_vocab_df['es'] == token.text])!=0: # not in 1000 list
            ez_token.append(token)
            words.append(es_vocab_df.iloc[es_vocab_df.index[es_vocab_df['es'] == token.text][0],1])
    if len(ez_token)!=0:
        num_pattern = re.compile(r"[0-9]")
        clean_tokens = [tok for tok in clean_tokens if tok in ez_token]
        selected_pos = ["VERB", "NOUN", "ADJ", "ADV"]
        clean_tokens = [tok for tok in clean_tokens if tok.pos_ in selected_pos]
        tokens_lemma_pos = [tok.lemma_ + " | " + tok.pos_ for tok in clean_tokens]
        vocab = list(set(tokens_lemma_pos))
        if vocab:
            selected_words = st.multiselect("è«‹é¸æ“‡è¦æŸ¥è©¢çš„å–®è©: ", vocab, vocab[0:3])
            for w in selected_words:
                word_pos = w.split("|")
                token = word_pos[0].strip()
                word = es_vocab_df.iloc[es_vocab_df.index[es_vocab_df['es'] == token][0],1]

                pos = word_pos[1].strip()
                st.write(f"### {w}")
                with st.expander("é»æ“Š + æª¢è¦–çµæœ"):
                    st.markdown(f"Like *{word}* in English")
                    show_definitions(word, pos)
    else:
        st.markdown("- no Elementary Spelling Words")

if morphology:
    st.markdown("## è©å½¢è®ŠåŒ–")
    # Collect inflected forms
    inflected_forms = [tok for tok in doc if tok.text.lower() != tok.lemma_.lower()]
    if inflected_forms:
        create_eng_df(inflected_forms)

if ner_viz:
    ner_labels = nlp.get_pipe("ner").labels
    visualize_ner(doc, labels=ner_labels, show_table=False, title="å‘½åå¯¦é«”")

if tok_table:
    visualize_tokens(doc, attrs=["text", "pos_", "tag_", "dep_", "head"], title="æ–·è©ç‰¹å¾µ")
