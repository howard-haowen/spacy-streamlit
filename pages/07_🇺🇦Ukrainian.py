import re

import pandas as pd
import requests
import spacy
# from spacy.language import Language
import streamlit as st
from bs4 import BeautifulSoup
from spacy_streamlit import visualize_ner, visualize_tokens

# Global variables
DEFAULT_TEXT = """Ğ¢Ğ¾Ğ¶ Ñ Ğ¿Ñ€Ğ¾Ğ¶Ğ¸Ğ² Ğ¶Ğ¸Ñ‚Ñ‚Ñ ÑĞ°Ğ¼, Ğ±ĞµĞ· Ğ½Ñ–ĞºĞ¾Ğ³Ğ¾, Ğ· ĞºĞ¸Ğ¼ Ğ±Ğ¸ Ğ¼Ñ–Ğ³ Ğ¿Ğ¾-ÑĞ¿Ñ€Ğ°Ğ²Ğ¶Ğ½ÑŒĞ¾Ğ¼Ñƒ Ğ¿Ğ¾Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ¸Ñ‚Ğ¸, Ğ´Ğ¾ĞºĞ¸ ÑˆÑ–ÑÑ‚ÑŒ Ñ€Ğ¾ĞºÑ–Ğ² Ñ‚Ğ¾Ğ¼Ñƒ Ğ½Ğµ Ğ·Ğ°Ğ·Ğ½Ğ°Ğ² Ğ°Ğ²Ğ°Ñ€Ñ–Ñ— Ğ· Ğ¼Ğ¾Ñ—Ğ¼ Ğ»Ñ–Ñ‚Ğ°ĞºĞ¾Ğ¼ Ñƒ Ğ¿ÑƒÑÑ‚ĞµĞ»Ñ– Ğ¡Ğ°Ñ…Ğ°Ñ€Ğ°. Ğ©Ğ¾ÑÑŒ Ğ·Ğ»Ğ°Ğ¼Ğ°Ğ»Ğ¾ÑÑ Ğ² Ğ¼Ğ¾Ñ”Ğ¼Ñƒ Ğ´Ğ²Ğ¸Ğ³ÑƒĞ½Ñ–. Ğ Ğ¾ÑĞºÑ–Ğ»ÑŒĞºĞ¸ Ğ·Ñ– Ğ¼Ğ½Ğ¾Ñ Ğ½Ğµ Ğ±ÑƒĞ»Ğ¾ Ğ°Ğ½Ñ– Ğ¼ĞµÑ…Ğ°Ğ½Ñ–ĞºĞ°, Ğ°Ğ½Ñ– Ğ¿Ğ°ÑĞ°Ğ¶Ğ¸Ñ€Ñ–Ğ², Ñ Ğ²Ğ·ÑĞ²ÑÑ Ğ·Ğ° Ğ²Ğ°Ğ¶ĞºĞ¸Ğ¹ Ñ€ĞµĞ¼Ğ¾Ğ½Ñ‚ ÑĞ°Ğ¼. Ğ”Ğ»Ñ Ğ¼ĞµĞ½Ğµ Ñ†Ğµ Ğ±ÑƒĞ»Ğ¾ Ğ¿Ğ¸Ñ‚Ğ°Ğ½Ğ½Ñ Ğ¶Ğ¸Ñ‚Ñ‚Ñ Ñ‡Ğ¸ ÑĞ¼ĞµÑ€Ñ‚Ñ–: Ğ¿Ğ¸Ñ‚Ğ½Ğ¾Ñ— Ğ²Ğ¾Ğ´Ğ¸ Ğ¼ĞµĞ½Ñ– Ğ½Ğµ Ğ²Ğ¸ÑÑ‚Ğ°Ñ‡Ğ°Ğ»Ğ¾ Ğ½Ğ° Ñ‚Ğ¸Ğ¶Ğ´ĞµĞ½ÑŒ. ĞÑ‚Ğ¶Ğµ, Ğ¿ĞµÑ€ÑˆĞ¾Ñ— Ğ½Ğ¾Ñ‡Ñ– Ñ Ğ»Ñ–Ğ³ ÑĞ¿Ğ°Ñ‚Ğ¸ Ğ½Ğ° Ğ¿Ñ–ÑĞºÑƒ, Ğ·Ğ° Ñ‚Ğ¸ÑÑÑ‡Ñƒ Ğ¼Ğ¸Ğ»ÑŒ Ğ²Ñ–Ğ´ Ğ±ÑƒĞ´ÑŒ-ÑĞºĞ¾Ğ³Ğ¾ Ğ»ÑĞ´ÑÑŒĞºĞ¾Ğ³Ğ¾ Ğ¶Ğ¸Ñ‚Ğ»Ğ°. Ğ¯ Ğ±ÑƒĞ² Ğ±Ñ–Ğ»ÑŒÑˆ Ñ–Ğ·Ğ¾Ğ»ÑŒĞ¾Ğ²Ğ°Ğ½Ğ¸Ğ¼, Ğ½Ñ–Ğ¶ Ğ¼Ğ¾Ñ€ÑĞº, ÑĞºĞ¸Ğ¹ Ğ·Ğ°Ğ·Ğ½Ğ°Ğ² ĞºĞ¾Ñ€Ğ°Ğ±ĞµĞ»ÑŒĞ½Ğ¾Ñ— Ğ°Ğ²Ğ°Ñ€Ñ–Ñ— Ğ½Ğ° Ğ¿Ğ»Ğ¾Ñ‚Ñƒ Ğ¿Ğ¾ÑĞµÑ€ĞµĞ´ Ğ¾ĞºĞµĞ°Ğ½Ñƒ. Ğ¢Ğ°ĞºĞ¸Ğ¼ Ñ‡Ğ¸Ğ½Ğ¾Ğ¼, Ğ²Ğ¸ Ğ¼Ğ¾Ğ¶ĞµÑ‚Ğµ ÑƒÑĞ²Ğ¸Ñ‚Ğ¸ Ğ¼Ğ¾Ñ” Ğ·Ğ´Ğ¸Ğ²ÑƒĞ²Ğ°Ğ½Ğ½Ñ Ğ½Ğ° ÑÑ…Ğ¾Ğ´Ñ– ÑĞ¾Ğ½Ñ†Ñ, ĞºĞ¾Ğ»Ğ¸ Ğ¼ĞµĞ½Ğµ Ñ€Ğ¾Ğ·Ğ±ÑƒĞ´Ğ¸Ğ² Ğ´Ğ¸Ğ²Ğ½Ğ¸Ğ¹ Ñ‚Ğ¸Ñ…Ğ¸Ğ¹ Ğ³Ğ¾Ğ»Ğ¾ÑĞ¾Ğº. Ğ¢Ğ°Ğ¼ Ğ±ÑƒĞ»Ğ¾ ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¾:
â€” Ğ‘ÑƒĞ´ÑŒ Ğ»Ğ°ÑĞºĞ°, Ğ½Ğ°Ğ¼Ğ°Ğ»ÑĞ¹ Ğ¼ĞµĞ½Ñ– Ğ²Ñ–Ğ²Ñ†Ñ!
"Ğ©Ğ¾!"
Â«ĞĞ°Ğ¼Ğ°Ğ»ÑĞ¹ Ğ¼ĞµĞ½Ñ– Ğ²Ñ–Ğ²Ñ†Ñ!Â»
ĞœĞ°Ğ»ĞµĞ½ÑŒĞºĞ¸Ğ¹ Ğ¿Ñ€Ğ¸Ğ½Ñ†
"""

DESCRIPTION = "AIæ¨¡å‹è¼”åŠ©èªè¨€å­¸ç¿’ï¼šçƒå…‹è˜­èª"
TOK_SEP = " | "
MODEL_NAME = "uk_core_news_sm"
API_LOOKUP = {}
MAX_SYM_NUM = 5


# External API caller
def free_dict_caller(word):
    req = requests.get(f"http://sum.in.ua/?swrd={word}")
    soup = BeautifulSoup(req.text, "lxml")
    try:
        article_body = soup.find(itemprop="articleBody")
        if article_body:
            content = article_body.contents
            lines = [tag.text for tag in content]
            API_LOOKUP[word] = "\n".join(lines)
    except:
        pass


def show_definitions_and_examples(word, pos):
    if word not in API_LOOKUP:
        free_dict_caller(word)

    result = API_LOOKUP.get(word)
    if result:
        st.markdown(result)

    else:
        st.info("Found no matching result on Free Dictionary!")


def get_synonyms(word, pos):
    if word not in API_LOOKUP:
        free_dict_caller(word)

    result = API_LOOKUP.get(word)
    if result:
        return result


# Utility functions
def create_eng_df(tokens):
    seen_texts = []
    filtered_tokens = []
    for tok in tokens:
        if tok.lemma_ not in seen_texts:
            filtered_tokens.append(tok)

    df = pd.DataFrame(
        {
            "å–®è©": [tok.text.lower() for tok in filtered_tokens],
            "è©é¡": [tok.pos_ for tok in filtered_tokens],
            "åŸå½¢": [tok.lemma_ for tok in filtered_tokens],
        }
    )
    st.dataframe(df)
    csv = df.to_csv().encode('utf-8')
    st.download_button(
        label="ä¸‹è¼‰è¡¨æ ¼",
        data=csv,
        file_name='eng_forms.csv',
    )


def filter_tokens(doc):
    clean_tokens = [tok for tok in doc if tok.pos_ not in ["PUNCT", "SYM"]]
    clean_tokens = [tok for tok in clean_tokens if not tok.like_email]
    clean_tokens = [tok for tok in clean_tokens if not tok.like_url]
    clean_tokens = [tok for tok in clean_tokens if not tok.like_num]
    clean_tokens = [tok for tok in clean_tokens if not tok.is_punct]
    clean_tokens = [tok for tok in clean_tokens if not tok.is_space]
    return clean_tokens


def create_kw_section(doc):
    st.markdown("## é—œéµè©åˆ†æ")
    kw_num = st.slider("è«‹é¸æ“‡é—œéµè©æ•¸é‡", 1, 10, 3)
    kws2scores = {keyword: score for keyword, score in doc._.extract_keywords(n=kw_num)}
    kws2scores = sorted(kws2scores.items(), key=lambda x: x[1], reverse=True)
    count = 1
    for keyword, score in kws2scores:
        rounded_score = round(score, 3)
        st.write(f"{count} >>> {keyword} ({rounded_score})")
        count += 1

    # Page setting


st.set_page_config(
    page_icon="ğŸ¤ ",
    layout="wide",
    initial_sidebar_state="auto",
)
st.markdown(f"# {DESCRIPTION}")

# Load the language model
nlp = spacy.load(MODEL_NAME)

# Add pipelines to spaCy
# nlp.add_pipe("yake")  # keyword extraction
# nlp.add_pipe("merge_entities") # Merge entity spans to tokens

# Page starts from here
st.markdown("## å¾…åˆ†ææ–‡æœ¬")
st.info("è«‹åœ¨ä¸‹é¢çš„æ–‡å­—æ¡†è¼¸å…¥æ–‡æœ¬ä¸¦æŒ‰ä¸‹Ctrl + Enterä»¥æ›´æ–°åˆ†æçµæœ")
text = st.text_area("", DEFAULT_TEXT, height=200)
doc = nlp(text)
st.markdown("---")

st.info("è«‹å‹¾é¸ä»¥ä¸‹è‡³å°‘ä¸€é …åŠŸèƒ½")
keywords_extraction = st.checkbox("é—œéµè©åˆ†æ", False)
analyzed_text = st.checkbox("å¢å¼·æ–‡æœ¬", True)
defs_examples = st.checkbox("å–®è©è§£æ", True)
morphology = st.checkbox("è©å½¢è®ŠåŒ–", False)
ner_viz = st.checkbox("å‘½åå¯¦é«”", True)
tok_table = st.checkbox("æ–·è©ç‰¹å¾µ", False)

if keywords_extraction:
    create_kw_section(doc)

if analyzed_text:
    st.markdown("## åˆ†æå¾Œæ–‡æœ¬")
    for idx, sent in enumerate(doc.sents):
        enriched_sentence = []
        for tok in sent:
            if tok.pos_ != "VERB":
                enriched_sentence.append(tok.text)
            else:
                synonyms = get_synonyms(tok.text, tok.pos_)
                if synonyms:
                    if len(synonyms) > MAX_SYM_NUM:
                        synonyms = synonyms[:MAX_SYM_NUM]
                    added_verbs = " | ".join(synonyms)
                    enriched_tok = f"{tok.text} (cf. {added_verbs})"
                    enriched_sentence.append(enriched_tok)
                else:
                    enriched_sentence.append(tok.text)

        display_text = " ".join(enriched_sentence)
        st.write(f"{idx + 1} >>> {display_text}")

if defs_examples:
    st.markdown("## å–®è©è§£é‡‹èˆ‡ä¾‹å¥")
    clean_tokens = filter_tokens(doc)
    num_pattern = re.compile(r"[0-9]")
    clean_tokens = [tok for tok in clean_tokens if not num_pattern.search(tok.lemma_)]
    selected_pos = ["VERB", "NOUN", "ADJ", "ADV"]
    clean_tokens = [tok for tok in clean_tokens if tok.pos_ in selected_pos]
    tokens_lemma_pos = [tok.lemma_ + " | " + tok.pos_ for tok in clean_tokens]
    vocab = list(set(tokens_lemma_pos))
    if vocab:
        selected_words = st.multiselect("è«‹é¸æ“‡è¦æŸ¥è©¢çš„å–®è©: ", vocab, vocab[0:3])
        for w in selected_words:
            word_pos = w.split("|")
            word = word_pos[0].strip()
            pos = word_pos[1].strip()
            st.write(f"### {w}")
            with st.expander("é»æ“Š + æª¢è¦–çµæœ"):
                show_definitions_and_examples(word, pos)

if morphology:
    st.markdown("## è©å½¢è®ŠåŒ–")
    # Collect inflected forms
    inflected_forms = [tok for tok in doc if tok.text.lower() != tok.lemma_.lower()]
    if inflected_forms:
        create_eng_df(inflected_forms)

if ner_viz:
    ner_labels = nlp.get_pipe("ner").labels
    visualize_ner(doc, labels=ner_labels, show_table=False, title="å‘½åå¯¦é«”")

if tok_table:
    visualize_tokens(doc, attrs=["text", "pos_", "tag_", "dep_", "head"], title="æ–·è©ç‰¹å¾µ")
